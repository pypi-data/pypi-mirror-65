{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import biject_to, transform_to\n",
    "import math\n",
    "import pyro.distributions as dist\n",
    "from pyro.distributions.lkj import LKJCorrCholesky, corr_cholesky_constraint, _signed_stick_breaking_tril\n",
    "\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 4\n",
    "d = LKJCorrCholesky(dimension=dimension, concentration=1)\n",
    "sample = d.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = LKJCorrCholesky(dimension=dimension, concentration=1)\n",
    "sample_shape = torch.Size([100])\n",
    "sample = d.sample(sample_shape)\n",
    "log_prob = d.log_prob(sample)\n",
    "\n",
    "# Now we will compute Jacobian of the transform from cholesky to correlation\n",
    "tril_index = sample.new_ones(sample.shape).tril(diagonal=-1) > 0.5\n",
    "sample_tril = sample[tril_index].clone().requires_grad_()\n",
    "sample_cloned = sample.new_zeros(sample.shape)\n",
    "sample_cloned_tmp = sample_cloned.clone()\n",
    "sample_cloned_tmp[tril_index] = sample_tril.reshape(-1)\n",
    "sample_cloned_diag = (1 - sample_cloned_tmp.pow(2).sum(-1)).sqrt()\n",
    "sample_cloned[tril_index] = sample_tril\n",
    "sample_cloned.view(sample_shape + (dimension * dimension,))[..., ::dimension + 1] = sample_cloned_diag\n",
    "y = sample_cloned.matmul(sample_cloned.transpose(-2, -1))\n",
    "corr_tril = y[tril_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.7084, 0.8603, 0.7924],\n",
       "        [1.0000, 0.9820, 0.7884, 0.4685],\n",
       "        [1.0000, 0.8950, 0.9178, 0.6933],\n",
       "        [1.0000, 0.9013, 0.8255, 0.6771],\n",
       "        [1.0000, 0.8084, 0.8430, 0.5885],\n",
       "        [1.0000, 0.5765, 0.5880, 0.5614],\n",
       "        [1.0000, 0.8189, 0.9792, 0.5258],\n",
       "        [1.0000, 0.9999, 0.8501, 0.7135],\n",
       "        [1.0000, 0.8813, 0.9052, 0.6342],\n",
       "        [1.0000, 0.7850, 0.7748, 0.5940],\n",
       "        [1.0000, 0.9863, 0.9019, 0.3314],\n",
       "        [1.0000, 0.6839, 0.7195, 0.4769],\n",
       "        [1.0000, 0.9318, 0.7580, 0.7520],\n",
       "        [1.0000, 0.9817, 0.4821, 0.7459],\n",
       "        [1.0000, 0.9834, 0.7387, 0.8711],\n",
       "        [1.0000, 0.9955, 0.8699, 0.4756],\n",
       "        [1.0000, 0.9231, 0.8307, 0.3116],\n",
       "        [1.0000, 0.8228, 0.9406, 0.6265],\n",
       "        [1.0000, 0.9969, 0.7438, 0.4063],\n",
       "        [1.0000, 0.9426, 0.7667, 0.4488],\n",
       "        [1.0000, 0.8246, 0.8403, 0.4185],\n",
       "        [1.0000, 0.9820, 0.9155, 0.3787],\n",
       "        [1.0000, 0.5453, 0.9216, 0.5922],\n",
       "        [1.0000, 0.9275, 0.6920, 0.5209],\n",
       "        [1.0000, 0.4688, 0.7967, 0.5130],\n",
       "        [1.0000, 0.6006, 0.8095, 0.9288],\n",
       "        [1.0000, 0.6767, 0.7809, 0.7639],\n",
       "        [1.0000, 0.9905, 0.8219, 0.7219],\n",
       "        [1.0000, 0.8916, 0.8905, 0.3929],\n",
       "        [1.0000, 0.8309, 0.9909, 0.7120],\n",
       "        [1.0000, 0.6726, 0.8384, 0.7344],\n",
       "        [1.0000, 0.8749, 0.4240, 0.2320],\n",
       "        [1.0000, 0.9049, 0.7984, 0.6386],\n",
       "        [1.0000, 0.8611, 0.9532, 0.4236],\n",
       "        [1.0000, 0.9754, 0.8233, 0.2090],\n",
       "        [1.0000, 0.9179, 0.9220, 0.6051],\n",
       "        [1.0000, 0.8464, 0.9655, 0.7049],\n",
       "        [1.0000, 0.5453, 0.7388, 0.8272],\n",
       "        [1.0000, 0.9990, 0.9091, 0.4557],\n",
       "        [1.0000, 0.8915, 0.6953, 0.5329],\n",
       "        [1.0000, 0.9923, 0.9423, 0.7545],\n",
       "        [1.0000, 0.8441, 0.9089, 0.7065],\n",
       "        [1.0000, 0.9011, 0.7116, 0.6513],\n",
       "        [1.0000, 0.8371, 0.6719, 0.8686],\n",
       "        [1.0000, 0.9950, 0.6679, 0.6786],\n",
       "        [1.0000, 0.7971, 0.7907, 0.7402],\n",
       "        [1.0000, 0.9434, 0.6821, 0.6833],\n",
       "        [1.0000, 0.7853, 0.6304, 0.4413],\n",
       "        [1.0000, 0.9748, 0.7460, 0.6142],\n",
       "        [1.0000, 0.9972, 0.9190, 0.7279],\n",
       "        [1.0000, 0.9770, 0.9088, 0.0915],\n",
       "        [1.0000, 0.9972, 0.4188, 0.5753],\n",
       "        [1.0000, 0.9787, 0.7000, 0.8571],\n",
       "        [1.0000, 0.9270, 0.4845, 0.7971],\n",
       "        [1.0000, 0.9802, 0.8375, 0.8050],\n",
       "        [1.0000, 0.9855, 0.6240, 0.8861],\n",
       "        [1.0000, 0.9948, 0.8995, 0.7038],\n",
       "        [1.0000, 0.5533, 0.9607, 0.4014],\n",
       "        [1.0000, 0.9987, 0.5081, 0.7099],\n",
       "        [1.0000, 0.7919, 0.7289, 0.6450],\n",
       "        [1.0000, 0.9178, 0.8918, 0.5835],\n",
       "        [1.0000, 0.9990, 0.8328, 0.6266],\n",
       "        [1.0000, 0.9356, 0.5573, 0.3515],\n",
       "        [1.0000, 0.6594, 0.8999, 0.9035],\n",
       "        [1.0000, 0.9852, 0.5713, 0.4866],\n",
       "        [1.0000, 0.9487, 0.8707, 0.7589],\n",
       "        [1.0000, 0.9489, 0.7608, 0.7120],\n",
       "        [1.0000, 0.9674, 0.9412, 0.6476],\n",
       "        [1.0000, 0.9979, 0.7143, 0.4784],\n",
       "        [1.0000, 0.5875, 0.6733, 0.4558],\n",
       "        [1.0000, 0.8953, 0.7753, 0.3928],\n",
       "        [1.0000, 0.9972, 0.8676, 0.5028],\n",
       "        [1.0000, 0.9484, 0.9667, 0.1473],\n",
       "        [1.0000, 0.7468, 0.8065, 0.6757],\n",
       "        [1.0000, 0.9938, 0.6943, 0.4378],\n",
       "        [1.0000, 0.8049, 0.6548, 0.5713],\n",
       "        [1.0000, 0.4852, 0.8515, 0.6022],\n",
       "        [1.0000, 0.9949, 0.6061, 0.6433],\n",
       "        [1.0000, 0.6500, 0.8836, 0.2235],\n",
       "        [1.0000, 0.8532, 0.8142, 0.4355],\n",
       "        [1.0000, 0.9019, 0.7309, 0.4906],\n",
       "        [1.0000, 0.7417, 0.7582, 0.4216],\n",
       "        [1.0000, 0.9974, 0.6223, 0.5093],\n",
       "        [1.0000, 0.8395, 0.7267, 0.5510],\n",
       "        [1.0000, 0.9402, 0.6753, 0.6983],\n",
       "        [1.0000, 0.9990, 0.4176, 0.6049],\n",
       "        [1.0000, 0.9665, 0.7850, 0.6358],\n",
       "        [1.0000, 0.9665, 0.8376, 0.7633],\n",
       "        [1.0000, 0.9876, 0.7639, 0.6641],\n",
       "        [1.0000, 0.5051, 0.6400, 0.8077],\n",
       "        [1.0000, 0.8480, 0.4984, 0.2125],\n",
       "        [1.0000, 0.8788, 0.7618, 0.9658],\n",
       "        [1.0000, 0.9806, 0.6837, 0.7734],\n",
       "        [1.0000, 0.9993, 0.9510, 0.7225],\n",
       "        [1.0000, 0.5983, 0.8686, 0.3062],\n",
       "        [1.0000, 0.8409, 0.9137, 0.5159],\n",
       "        [1.0000, 0.9129, 0.7565, 0.2894],\n",
       "        [1.0000, 0.9066, 0.5107, 0.7264],\n",
       "        [1.0000, 0.9672, 0.4338, 0.8367],\n",
       "        [1.0000, 0.6928, 0.9613, 0.5887]], grad_fn=<SqrtBackward>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_cloned_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 4, 4])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_cloned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 4, 4])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_cloned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.4594)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(-2.4594)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_tril = y[tril_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1.6547), tensor(-0.0584))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.8193)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.log_prob(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2230)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_autograd_log_det(corr_tril, sample_tril)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.0794415416798357"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-3 * math.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.matmul(x.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-3d4faf8e1b9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_autograd_log_det\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtril_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtril_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-e0353dbbce4d>\u001b[0m in \u001b[0;36m_autograd_log_det\u001b[0;34m(ys, x)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# computes log_abs_det_jacobian of y w.r.t. x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     return torch.stack([torch.autograd.grad(y, (x,), retain_graph=True)[0]\n\u001b[0;32m----> 4\u001b[0;31m                         for y in ys]).slogdet()[1]\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-e0353dbbce4d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# computes log_abs_det_jacobian of y w.r.t. x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     return torch.stack([torch.autograd.grad(y, (x,), retain_graph=True)[0]\n\u001b[0;32m----> 4\u001b[0;31m                         for y in ys]).slogdet()[1]\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/pydata/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    143\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    144\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "_autograd_log_det(y[tril_index], x[tril_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tril = sample[tril_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _autograd_log_det(ys, x):\n",
    "    # computes log_abs_det_jacobian of y w.r.t. x\n",
    "    return torch.stack([torch.autograd.grad(y, (x,), retain_graph=True)[0]\n",
    "                        for y in ys]).slogdet()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 4\n",
    "concentration = torch.tensor(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = LKJCorrCholesky(dimension, concentration, sample_method=\"cvine\")\n",
    "sample = d.sample()\n",
    "\n",
    "# Start with the lower triangular part of a sample, then we will transform it back to a\n",
    "# partial correlation; compute its log_prob and Jacobian of the transfrom.\n",
    "tril_index = sample.new_ones(dimension, dimension).tril(diagonal=-1) > 0.5\n",
    "sample_tril = sample[tril_index].clone().requires_grad_()\n",
    "sample_cloned = sample.new_ones(dimension, dimension).tril(diagonal=-1)\n",
    "sample_cloned_tmp = sample_cloned.clone()\n",
    "sample_cloned_tmp[tril_index] = sample_tril\n",
    "sample_cloned_diag = (1 - sample_cloned_tmp.pow(2).sum(-1)).sqrt()\n",
    "sample_cloned[tril_index] = sample_tril\n",
    "sample_cloned.view(-1)[::dimension + 1] = sample_cloned_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0528, 0.9986, 0.0000, 0.0000],\n",
       "        [0.4672, 0.0638, 0.8819, 0.0000],\n",
       "        [0.5898, 0.2748, 0.0927, 0.7537]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0528, 0.9986, 0.0000, 0.0000],\n",
       "        [0.4672, 0.0638, 0.8819, 0.0000],\n",
       "        [0.5898, 0.2748, 0.0927, 0.7537]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_cloned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0528, 0.9986, 0.0000, 0.0000],\n",
       "        [0.4672, 0.0638, 0.8819, 0.0000],\n",
       "        [0.5898, 0.2748, 0.0927, 0.7537]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9588, grad_fn=<SumBackward2>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_corr = transform_to(corr_cholesky_constraint).inv(sample_cloned).tanh()\n",
    "beta_sample = (partial_corr + 1) / 2  # inverse affine transform\n",
    "partial_corr_log_prob = d._beta_dist.log_prob(beta_sample).sum(-1)\n",
    "partial_corr_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.5467)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_autograd_log_det(beta_sample, sample_tril)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.1572)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.log_prob(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.5879, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_log_prob = partial_corr_log_prob + _autograd_log_det(beta_sample, sample_tril)\n",
    "target_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0014)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1, 1].log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = sample\n",
    "order_offset = torch.arange(4 - dimension, 2.1)\n",
    "order = 2 * concentration.unsqueeze(-1) - order_offset\n",
    "\n",
    "# Compute unnormalized log_prob:\n",
    "cholesky_logprob = (order * value.diagonal(dim1=-2, dim2=-1)[..., 1:].log()).sum(-1)\n",
    "\n",
    "# Compute normalization constant (on the first proof of page 1999 of [1])\n",
    "denominator_concentration = concentration + (dimension - 1) / 2.\n",
    "denominator = torch.lgamma(denominator_concentration) * (dimension - 1)\n",
    "numerator = torch.mvlgamma(denominator_concentration - 0.5, dimension - 1)\n",
    "# pi_constant in [1] is D * (D - 1) / 4\n",
    "# pi_constant in torch.mvlgamma is (D - 1) * (D - 2) / 4\n",
    "# hence, we need to add a pi_constant = (D - 1) * (1 - D/4)\n",
    "pi_constant = (dimension - 1) / 2. * math.log(math.pi)\n",
    "normalization_constant = pi_constant + numerator - denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4594)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalization_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4594)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerator = (torch.lgamma(concentration + 1) + torch.lgamma(concentration + 0.5) +\n",
    "             torch.lgamma(concentration) - torch.lgamma(concentration + 1.5) -\n",
    "             torch.lgamma(concentration + 1.5) - torch.lgamma(concentration + 1.5) +\n",
    "             math.log(math.pi) * 3 / 2 + math.log(math.pi) * 1 / 2 + math.log(math.pi) * 2 / 2)\n",
    "numerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.lgamma(concentration + 0.5) + torch.lgamma(concentration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
       "        [ 0.0276,  0.0000,  1.0000,  1.0000,  1.0000],\n",
       "        [ 0.0949, -0.4510,  0.0000,  1.0000,  1.0000],\n",
       "        [ 0.0035,  0.1989, -0.6447,  0.0000,  1.0000],\n",
       "        [ 0.6909, -0.0108, -0.3325, -0.0022,  0.0000]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0276,  0.0949, -0.4510,  0.0035,  0.1989, -0.6447,  0.6909, -0.0108,\n",
       "        -0.3325, -0.0022], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.0000,  3.0000,  1.5490,  0.5542, -0.3455],\n",
       "        [ 3.0000,  3.0008,  2.0026,  0.3554, -0.3156],\n",
       "        [ 1.5490,  2.0026,  2.2124,  0.9106,  0.0682],\n",
       "        [ 0.5542,  0.3554,  0.9106,  1.4552,  0.2146],\n",
       "        [-0.3455, -0.3156,  0.0682,  0.2146,  0.5880]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-inf)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = LKJCorrCholesky(80, torch.rand(1), sample_method=\"onion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.new_ones(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4958, 0.5597, 0.2474],\n",
       "        [0.3369, 0.6387, 0.4935],\n",
       "        [0.4567, 0.1401, 0.6528],\n",
       "        [0.4648, 0.3759, 0.9469],\n",
       "        [0.1449, 0.3248, 0.0386],\n",
       "        [0.1499, 0.7719, 0.9055],\n",
       "        [0.2691, 0.9263, 0.3220],\n",
       "        [0.8208, 0.8923, 0.4726],\n",
       "        [0.9984, 0.2575, 0.1213],\n",
       "        [0.5694, 0.6186, 0.5278]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diagonal(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[3, 1, 80]}, size=[3, 80]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-01de2d95469c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/pydata/lib/python3.6/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \"\"\"\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyro/pyro/distributions/lkj.py\u001b[0m in \u001b[0;36mrsample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rsample_cvine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rsample_onion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_rsample_cvine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pyro/pyro/distributions/lkj.py\u001b[0m in \u001b[0;36m_rsample_onion\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mcholesky\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtril_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mcholesky_diag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcholesky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mcholesky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mD\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcholesky_diag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcholesky\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[3, 1, 80]}, size=[3, 80]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"
     ]
    }
   ],
   "source": [
    "d.sample(sample_shape=torch.Size([3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/fehiepsi/pyro/pyro/distributions/lkj.py\u001b[0m(311)\u001b[0;36m_rsample_onion\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    308 \u001b[0;31m        \u001b[0mtril_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcholesky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_ones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcholesky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    309 \u001b[0;31m        \u001b[0mcholesky\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtril_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    310 \u001b[0;31m        \u001b[0mcholesky_diag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcholesky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 311 \u001b[0;31m        \u001b[0mcholesky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mD\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcholesky_diag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    312 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mcholesky\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  cholesky_diag.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 80])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  cholesky.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 80, 80])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "x.diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 80, 80])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/fehiepsi/pyro/pyro/distributions/lkj.py\u001b[0m(299)\u001b[0;36m_rsample_onion\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    297 \u001b[0;31m        \u001b[0mnormal_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    298 \u001b[0;31m        \u001b[0msphere_uniform_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormal_sample\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnormal_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 299 \u001b[0;31m        \u001b[0mtril_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    300 \u001b[0;31m        \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msphere_uniform_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtril_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    301 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  beta_sample.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9480])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  sphere_uniform_sample.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 79, 79])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  79 * 79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6241\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  80 * 79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6320\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  normal_sample.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 79, 79])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  loc.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 79, 79])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self._beta_dist.batch_shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9480])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = LKJCorrCholesky(80, 1, sample_method=\"cvine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21 s, sys: 1.31 s, total: 22.3 s\n",
      "Wall time: 8.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = d.sample(torch.Size([5000]))\n",
    "y = x.matmul(x.transpose(-1, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(10, 10).tril(diagonal=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.1770, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.9371, 0.0278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.9500, 0.8768, 0.1039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.4127, 0.4784, 0.0989, 0.2805, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.6206, 0.4922, 0.1069, 0.4032, 0.1080, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.1665, 0.3607, 0.4326, 0.2511, 0.0284, 0.7881, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.8304, 0.9402, 0.2872, 0.5601, 0.7791, 0.9055, 0.9199, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.8698, 0.4121, 0.9474, 0.3823, 0.1347, 0.5006, 0.9804, 0.2945, 0.0000,\n",
       "         0.0000],\n",
       "        [0.8448, 0.8238, 0.7491, 0.0591, 0.5597, 0.2987, 0.7390, 0.1199, 0.5501,\n",
       "         0.0000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/fehiepsi/pyro/pyro/distributions/lkj.py\u001b[0m(52)\u001b[0;36m_signed_stick_breaking_tril\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     50 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m    \u001b[0;31m# transform t to tril matrix with identity diagonal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 52 \u001b[0;31m    \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_ones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiagonal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     53 \u001b[0;31m    \u001b[0mtril_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     54 \u001b[0;31m    \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtril_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  t.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100000, 45])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  t.device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device(type='cuda', index=0)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  t.new_ones(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  t.new_ones(t.shape[:-1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.,  ..., 1., 1., 1.])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  t.new_ones(t.shape[:-1] + (D, D))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  t.new_ones(t.shape[:-1] + (D, D)).tril(diagonal=-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** RuntimeError: CUDA error: invalid configuration argument\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  t.new_ones(t.shape[:-1] + (D, D)).shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100000, 10, 10])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit()\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = x.matmul(x.transpose(-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 3.1614e-01, 3.1634e-01, 3.1656e-01, 3.1553e-01, 3.1650e-01,\n",
       "         3.1666e-01, 3.1687e-01, 3.1708e-01, 3.1792e-01],\n",
       "        [3.1614e-01, 9.1151e-17, 3.1555e-01, 3.1533e-01, 3.1601e-01, 3.1601e-01,\n",
       "         3.1606e-01, 3.1626e-01, 3.1506e-01, 3.1666e-01],\n",
       "        [3.1634e-01, 3.1555e-01, 1.4001e-16, 3.1707e-01, 3.1603e-01, 3.1666e-01,\n",
       "         3.1648e-01, 3.1639e-01, 3.1561e-01, 3.1669e-01],\n",
       "        [3.1656e-01, 3.1533e-01, 3.1707e-01, 1.0386e-16, 3.1578e-01, 3.1637e-01,\n",
       "         3.1560e-01, 3.1628e-01, 3.1621e-01, 3.1538e-01],\n",
       "        [3.1553e-01, 3.1601e-01, 3.1603e-01, 3.1578e-01, 1.1097e-16, 3.1534e-01,\n",
       "         3.1585e-01, 3.1727e-01, 3.1732e-01, 3.1606e-01],\n",
       "        [3.1650e-01, 3.1601e-01, 3.1666e-01, 3.1637e-01, 3.1534e-01, 1.1836e-16,\n",
       "         3.1649e-01, 3.1514e-01, 3.1581e-01, 3.1619e-01],\n",
       "        [3.1666e-01, 3.1606e-01, 3.1648e-01, 3.1560e-01, 3.1585e-01, 3.1649e-01,\n",
       "         1.2413e-16, 3.1631e-01, 3.1643e-01, 3.1585e-01],\n",
       "        [3.1687e-01, 3.1626e-01, 3.1639e-01, 3.1628e-01, 3.1727e-01, 3.1514e-01,\n",
       "         3.1631e-01, 1.6535e-16, 3.1499e-01, 3.1625e-01],\n",
       "        [3.1708e-01, 3.1506e-01, 3.1561e-01, 3.1621e-01, 3.1732e-01, 3.1581e-01,\n",
       "         3.1643e-01, 3.1499e-01, 1.7081e-16, 3.1703e-01],\n",
       "        [3.1792e-01, 3.1666e-01, 3.1669e-01, 3.1538e-01, 3.1606e-01, 3.1619e-01,\n",
       "         3.1585e-01, 3.1625e-01, 3.1703e-01, 1.7860e-16]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 4\n",
    "x = torch.ones(D * (D-1) // 2, requires_grad=True)\n",
    "y = x.new_zeros(D, D)\n",
    "tril_index = y.new_ones(D, D).tril(diagonal=-1) > 0.5\n",
    "y[tril_index] = x\n",
    "y.view(-1)[::D+1] = y.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [-0.4663,  0.8846,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.6254,  0.3690,  0.6875,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.4112,  0.0647, -0.4068,  0.8131,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.3380, -0.0882, -0.0610, -0.7045,  0.6147,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [-0.2838, -0.2251, -0.4623,  0.2745, -0.1423,  0.7480,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [-0.0847,  0.1818,  0.2294, -0.4110,  0.3110, -0.3466,  0.7220,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [-0.3346,  0.3746, -0.3475,  0.3819, -0.4790,  0.0401, -0.4967,  0.0581,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0861,  0.5125, -0.1190, -0.0087, -0.4350,  0.3188,  0.4801,  0.2605,\n",
       "          0.3555,  0.0000],\n",
       "        [ 0.0688, -0.3106,  0.0371, -0.7020, -0.1260,  0.0388,  0.3516, -0.3275,\n",
       "         -0.1648,  0.3594]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 2., 0.],\n",
       "        [1., 1., 1., 3.]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(10).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.cuda.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.new_ones(x.shape + (10,)).tril()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3162)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.Beta(4.5, 4.5).variance.sqrt() * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 3.1614e-01, 3.1634e-01, 3.1656e-01, 3.1553e-01, 3.1650e-01,\n",
       "         3.1666e-01, 3.1687e-01, 3.1708e-01, 3.1792e-01],\n",
       "        [3.1614e-01, 9.1151e-17, 3.1555e-01, 3.1533e-01, 3.1601e-01, 3.1601e-01,\n",
       "         3.1606e-01, 3.1626e-01, 3.1506e-01, 3.1666e-01],\n",
       "        [3.1634e-01, 3.1555e-01, 1.4001e-16, 3.1707e-01, 3.1603e-01, 3.1666e-01,\n",
       "         3.1648e-01, 3.1639e-01, 3.1561e-01, 3.1669e-01],\n",
       "        [3.1656e-01, 3.1533e-01, 3.1707e-01, 1.0386e-16, 3.1578e-01, 3.1637e-01,\n",
       "         3.1560e-01, 3.1628e-01, 3.1621e-01, 3.1538e-01],\n",
       "        [3.1553e-01, 3.1601e-01, 3.1603e-01, 3.1578e-01, 1.1097e-16, 3.1534e-01,\n",
       "         3.1585e-01, 3.1727e-01, 3.1732e-01, 3.1606e-01],\n",
       "        [3.1650e-01, 3.1601e-01, 3.1666e-01, 3.1637e-01, 3.1534e-01, 1.1836e-16,\n",
       "         3.1649e-01, 3.1514e-01, 3.1581e-01, 3.1619e-01],\n",
       "        [3.1666e-01, 3.1606e-01, 3.1648e-01, 3.1560e-01, 3.1585e-01, 3.1649e-01,\n",
       "         1.2413e-16, 3.1631e-01, 3.1643e-01, 3.1585e-01],\n",
       "        [3.1687e-01, 3.1626e-01, 3.1639e-01, 3.1628e-01, 3.1727e-01, 3.1514e-01,\n",
       "         3.1631e-01, 1.6535e-16, 3.1499e-01, 3.1625e-01],\n",
       "        [3.1708e-01, 3.1506e-01, 3.1561e-01, 3.1621e-01, 3.1732e-01, 3.1581e-01,\n",
       "         3.1643e-01, 3.1499e-01, 1.7081e-16, 3.1703e-01],\n",
       "        [3.1792e-01, 3.1666e-01, 3.1669e-01, 3.1538e-01, 3.1606e-01, 3.1619e-01,\n",
       "         3.1585e-01, 3.1625e-01, 3.1703e-01, 1.7860e-16]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -1.5795e-11, -1.0260e-03, -4.4503e-04],\n",
       "        [-1.5795e-11,  1.0000e+00, -5.3766e-04, -1.5094e-03],\n",
       "        [-1.0260e-03, -5.3766e-04,  1.0000e+00,  2.4660e-03],\n",
       "        [-4.4503e-04, -1.5094e-03,  2.4660e-03,  1.0000e+00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_sample = d._beta_dist.rsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.2204e-16, 8.7688e-01, 1.6567e-01, 8.0272e-01, 9.2591e-01, 1.9920e-01])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0000, 1.5000, 1.5000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d._beta_dist.concentration0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5000, 0.5000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d._beta_dist.concentration1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = beta_sample.new_zeros(beta_sample.shape[:-1] + (D - 1, D - 1))\n",
    "scale = loc.new_ones(loc.shape)\n",
    "normal_sample = torch.normal(loc, scale).tril()\n",
    "sphere_uniform_sample = normal_sample / normal_sample.norm(dim=-1, keepdim=True)\n",
    "tril_index = scale.tril() > 0.5\n",
    "w = beta_sample.sqrt().reshape(-1) * sphere_uniform_sample[tril_index]\n",
    "\n",
    "# Note that w is the triangular part of a Cholesky factor of a correlation\n",
    "# matrix (from the procedure in algorithm 3.2 of [1]).\n",
    "# The diagonal entries of Cholesky factor is sqrt(1 - w^2). We can show it by linear\n",
    "# algebra or by recalling that each row of Cholesky factor has unit Euclidean length.\n",
    "cholesky = beta_sample.new_zeros(beta_sample.shape[:-1] + (D, D))\n",
    "tril_index = cholesky.new_ones(cholesky.shape).tril(diagonal=-1) > 0.5\n",
    "cholesky[tril_index] = w\n",
    "cholesky_diag = (1 - cholesky.pow(2).sum(-1)).sqrt()\n",
    "cholesky.view(-1, D * D)[..., ::D + 1] = cholesky_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "arange() received an invalid combination of arguments - got (step=float, end=float, ), but expected one of:\n * (Number end, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)\n * (Number start, Number end, Number step, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5d2fabc8c414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: arange() received an invalid combination of arguments - got (step=float, end=float, ), but expected one of:\n * (Number end, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)\n * (Number start, Number end, Number step, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "torch.arange(end=10., step=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/fehiepsi/pyro/pyro/distributions/lkj.py\u001b[0m(205)\u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    203 \u001b[0;31m            beta_concentration_offset = torch.arange((dimension - 1.5) / 2, step=0.5,\n",
      "\u001b[0m\u001b[0;32m    204 \u001b[0;31m                                                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcentration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 205 \u001b[0;31m                                                     device=concentration.device)\n",
      "\u001b[0m\u001b[0;32m    206 \u001b[0;31m            \u001b[0mbeta_concentration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta_concentration_init\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta_concentration_offset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    207 \u001b[0;31m            \u001b[0;31m# expand to a matrix then takes the vector form of the lower triangular part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  dimension\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = LKJCorrCholesky(10, 1, sample_method=\"C-vine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = d(torch.Size([8000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.3373,  0.9414,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.5406, -0.1343,  0.8305,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [-0.1373,  0.0849,  0.2268,  ...,  0.6343,  0.0000,  0.0000],\n",
       "         [-0.1566,  0.1786,  0.1847,  ...,  0.0589,  0.8067,  0.0000],\n",
       "         [ 0.3151, -0.3075,  0.0927,  ..., -0.4077,  0.2942,  0.2706]],\n",
       "\n",
       "        [[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.5596,  0.8287,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.3119,  0.1231,  0.9421,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.1565,  0.3144,  0.7781,  ...,  0.4808,  0.0000,  0.0000],\n",
       "         [-0.2162, -0.1328,  0.4667,  ..., -0.4097,  0.5027,  0.0000],\n",
       "         [ 0.2822,  0.2354,  0.0455,  ..., -0.0159,  0.1627,  0.3905]],\n",
       "\n",
       "        [[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0121,  0.9999,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.2996, -0.2617,  0.9175,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [-0.2407, -0.0106,  0.5926,  ...,  0.6074,  0.0000,  0.0000],\n",
       "         [-0.3481, -0.1575,  0.0161,  ...,  0.5579,  0.4332,  0.0000],\n",
       "         [ 0.3386,  0.1394,  0.3687,  ..., -0.0312, -0.0326,  0.2458]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0832,  0.9965,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0085,  0.2142,  0.9768,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.1146,  0.1736, -0.0723,  ...,  0.5406,  0.0000,  0.0000],\n",
       "         [-0.5735,  0.3350, -0.4363,  ..., -0.2875,  0.4379,  0.0000],\n",
       "         [ 0.3450, -0.1987, -0.0169,  ..., -0.1941, -0.5096,  0.4774]],\n",
       "\n",
       "        [[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0270,  0.9996,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.3054,  0.1886,  0.9334,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.6954, -0.1353, -0.1484,  ...,  0.5610,  0.0000,  0.0000],\n",
       "         [ 0.2778, -0.0449,  0.3201,  ...,  0.1881,  0.5040,  0.0000],\n",
       "         [ 0.3932, -0.0338, -0.3650,  ...,  0.3862,  0.0945,  0.4784]],\n",
       "\n",
       "        [[ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.2473,  0.9689,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.3468,  0.0809,  0.9345,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [-0.2605, -0.2261,  0.0658,  ...,  0.5507,  0.0000,  0.0000],\n",
       "         [-0.0597,  0.5665, -0.0973,  ...,  0.4180,  0.4858,  0.0000],\n",
       "         [ 0.1995, -0.3923,  0.6050,  ...,  0.0980,  0.3993,  0.3571]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 ms  1.73 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "samples = d(torch.Size([8000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264 ms  6.13 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "samples = f(torch.Size([8000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 10\n",
    "y = torch.rand(D, D).tril()\n",
    "y = y / y.norm(dim=-1, keepdim=True)\n",
    "x = t.inv(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit t.inv(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 80\n",
    "x = torch.rand(5000, D * (D - 1) // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(0.).cosh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    # make sure that x.size(-1) = D * (D - 1) / 2 for some D\n",
    "    D = round((1 + math.sqrt(1 + 8 * x.size(-1))) / 2)\n",
    "    if D * (D - 1) != 2 * x.size(-1):\n",
    "        raise ValueError(\"This transformation requires an input with last shape is \"\n",
    "                         \"D*(D-1)/2 for some integer D.\")\n",
    "\n",
    "    # we interchange step 1 and step 2.a for a better performance\n",
    "    eps = torch.finfo(x.dtype).eps\n",
    "    t = x.tanh().clamp(min=(-1 + eps), max=(1 - eps))\n",
    "\n",
    "    # transform to tril matrix with identity diagonal\n",
    "    r = t.new_ones(t.shape[:-1] + (D, D)).tril(diagonal=-1)\n",
    "    tril_index = r > 0.5\n",
    "    r[tril_index] = t.reshape(-1)\n",
    "    r.view(-1, D * D)[..., ::D + 1] = 1\n",
    "\n",
    "    # apply stick-breaking on the squared values;\n",
    "    # we omit the step of computing s = z * z_cumprod by using the fact:\n",
    "    #     y = sign(r) * s = sign(r) * sqrt(z * z_cumprod) = r * sqrt(z_cumprod)\n",
    "    z = r ** 2\n",
    "    z_cumprod = (1 - z).cumprod(-1)\n",
    "\n",
    "    # to workaround the issue: NaN propagated through backward pass even when not accessed\n",
    "    # at https://github.com/pytorch/pytorch/issues/15506,\n",
    "    # here we only take sqrt at tril_index\n",
    "    z_cumprod_sqrt = z_cumprod.new_zeros(z_cumprod.shape)\n",
    "    z_cumprod_sqrt[tril_index] = z_cumprod[tril_index].sqrt()\n",
    "    z_cumprod_sqrt_shifted = F.pad(z_cumprod_sqrt[..., :-1], pad=(1, 0), value=1)\n",
    "    y = r * z_cumprod_sqrt_shifted\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    # make sure that x.size(-1) = D * (D - 1) / 2 for some D\n",
    "    D = round((1 + math.sqrt(1 + 8 * x.size(-1))) / 2)\n",
    "    if D * (D - 1) != 2 * x.size(-1):\n",
    "        raise ValueError(\"This transformation requires an input with last shape is \"\n",
    "                         \"D*(D-1)/2 for some integer D.\")\n",
    "\n",
    "    # we interchange step 1 and step 2.a for a better performance\n",
    "    eps = torch.finfo(x.dtype).eps\n",
    "    t = x.tanh().clamp(min=(-1 + eps), max=(1 - eps))\n",
    "\n",
    "    # transform to tril matrix with identity diagonal\n",
    "    r = t.new_ones(t.shape[:-1] + (D, D)).tril(diagonal=-1)\n",
    "    tril_index = r > 0.5\n",
    "    r[tril_index] = t.reshape(-1)\n",
    "    r.view(-1, D * D)[..., ::D + 1] = 1\n",
    "\n",
    "    # apply stick-breaking on the squared values;\n",
    "    # we omit the step of computing s = z * z_cumprod by using the fact:\n",
    "    #     y = sign(r) * s = sign(r) * sqrt(z * z_cumprod) = r * sqrt(z_cumprod)\n",
    "    z = r ** 2\n",
    "    z_cumprod = (1 - z).cumprod(-1)\n",
    "\n",
    "    # to workaround the issue: NaN propagated through backward pass even when not accessed\n",
    "    # at https://github.com/pytorch/pytorch/issues/15506,\n",
    "    # here we only take sqrt at tril_index\n",
    "    z_cumprod_sqrt = z_cumprod.new_zeros(z_cumprod.shape)\n",
    "    z_cumprod_sqrt[tril_index] = z_cumprod[tril_index].sqrt()\n",
    "    z_cumprod_sqrt_shifted = F.pad(z_cumprod_sqrt[..., :-1], pad=(1, 0), value=1)\n",
    "    y = z_cumprod.new_zeros(z_cumprod.shape)\n",
    "    y[tril_index] = t.reshape(-1) * z_cumprod_sqrt_shifted[tril_index]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.81 s  21.2 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.89 s  160 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit g(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.8 s  77.5 ns per loop (mean  std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit (1 + x) / (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1 s  79.7 ns per loop (mean  std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit 1 - (1 - x).reciprocal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 s  51.1 ns per loop (mean  std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  2 / (1 - x) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit t.log_abs_det_jacobian(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform = _PartialCorrToCorrCholeskyTransform()\n",
    "x_shape = (6,)\n",
    "x = torch.empty(x_shape).uniform_(-1, 1).requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def _vector_to_l_cholesky(z):\n",
    "    D = (1.0 + math.sqrt(1.0 + 8.0 * z.shape[-1]))/2.0\n",
    "    if D % 1 != 0:\n",
    "        raise ValueError(\"Correlation matrix transformation requires d choose 2 inputs\")\n",
    "    D = int(D)\n",
    "    x = torch.zeros(list(z.shape[:-1]) + [D,D], device=z.device)\n",
    "\n",
    "    x[..., 0,0] = 1\n",
    "    x[..., 1:,0] = z[..., :(D-1)]\n",
    "    i = D - 1\n",
    "    last_squared_x = torch.zeros(list(z.shape[:-1]) + [D], device=z.device)\n",
    "    for j in range(1, D):\n",
    "        distance_to_copy = D - 1 - j\n",
    "        last_squared_x = last_squared_x[..., 1:] + x[...,j:,(j-1)].clone()**2\n",
    "        x[..., j, j] = (1 - last_squared_x[..., 0]).sqrt()\n",
    "        x[..., (j+1):, j] = z[..., i:(i + distance_to_copy)] * (1 - last_squared_x[..., 1:]).sqrt()\n",
    "        i += distance_to_copy\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def _call(x):\n",
    "    # make sure that x.size(-1) = D * (D - 1) / 2 for some D\n",
    "    D = round((1 + math.sqrt(1 + 8 * x.size(-1))) / 2)\n",
    "    if D * (D - 1) != 2 * x.size(-1):\n",
    "        raise ValueError(\"This transformation requires an input with last shape is \"\n",
    "                         \"D*(D-1)/2 for some integer D.\")\n",
    "\n",
    "    # transform to tril matrix\n",
    "    r = x.new_ones(x.shape[:-1] + (D, D)).tril()\n",
    "    tril_index = r.tril(diagonal=-1) > 0.5\n",
    "    r[tril_index] = x.reshape(-1)\n",
    "\n",
    "    # apply stick-breaking on the squared values\n",
    "    z = r ** 2\n",
    "    z_cumprod = (1 - z).cumprod(-1)\n",
    "    # we omit the step computing s = z * z_cumprod by using the trick:\n",
    "    #     y = sign(r) * s = sign(r) * sqrt(z * z_cumprod) = r * sqrt(z_cumprod)\n",
    "\n",
    "    # workaround the issue: NaN propagated through backward pass even when not accessed\n",
    "    # at https://github.com/pytorch/pytorch/issues/15506;\n",
    "    # here we only take sqrt at tril_index\n",
    "    z_cumprod_sqrt = z_cumprod.new_zeros(z_cumprod.shape)\n",
    "    z_cumprod_sqrt[tril_index] = z_cumprod[tril_index].sqrt()        \n",
    "    z_cumprod_sqrt_shifted = F.pad(z_cumprod_sqrt[..., :-1], pad=(1, 0), value=1)\n",
    "    y = r * z_cumprod_sqrt_shifted\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit torch.arange(beta_c_init - 0.5, beta_c_init - D / 2. + 0.1, 0.5).expand(D, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.arange(1., 6, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = torch.arange(1., 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit 2 * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit torch.arange(2., 20, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.finfo(torch.float).epsneg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.finfo(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D = 3\n",
    "x = torch.randn(D * (D-1) // 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit _call(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.set_printoptions(10)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = torch.cuda.FloatTensor([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.finfo(x.dtype).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.tanh(torch.tensor(-10.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.sigmoid(torch.tensor(-40.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.tensor(-40.).tanh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.rand(10).expand(torch.Size([3, 3, -1])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit _vector_to_l_cholesky(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def _call(x):\n",
    "    # transform to tril matrix\n",
    "    r = _vector_to_lower_triangular_with_identity_diagonal(x)\n",
    "\n",
    "    # apply stick-breaking on the squared values\n",
    "    z = r ** 2\n",
    "    z_cumprod = (1 - z).cumprod(-1)\n",
    "\n",
    "    # we omit the step to compute s = z * z_cumprod by using the trick:\n",
    "    #     y = sign(r) * s = sign(r) * sqrt(z * z_cumprod) = r * sqrt(z_cumprod)\n",
    "    y = r * F.pad(_masked_sqrt(z_cumprod[..., :-1]), pad=(1, 0), value=1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tril_index = x.new_ones(x.shape).tril(diagonal=diagonal) > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.uint8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.full((4, 4), 0.5).cumsum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = torch.rand(3, 3).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.tensor(3.).size(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x.new_ones((3, 3), dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def abc(x):\n",
    "    # make sure that x.size(-1) = D * (D - 1) / 2 for some D\n",
    "    D = (1 + math.sqrt(1 + 8 * x.size(-1))) / 2\n",
    "    if D % 1 != 0:\n",
    "        raise ValueError(\"This transformation requires an input with size D*(D-1)/2.\")\n",
    "    D = int(D)\n",
    "    r = x.new_ones(x.shape[:-1] + (D, D)).tril()\n",
    "    tril_index = r.tril(diagonal=-1) > 0.5\n",
    "    r[tril_index] = x.reshape(-1)\n",
    "    return r, tril_index\n",
    "\n",
    "\n",
    "def defg(x, mask):\n",
    "    # workaround the issue NaN propagated through backward pass even when not accessed\n",
    "    # https://github.com/pytorch/pytorch/issues/15506\n",
    "    y = x.new_zeros(x.shape)\n",
    "    y[mask] = x[mask].sqrt()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D = 10\n",
    "x = torch.rand(10, D * (D - 1) // 2)\n",
    "value = torch.randn(10, D, D).tril()\n",
    "value.diagonal(dim1=-2, dim2=-1).exp_()\n",
    "y = value / value.norm(2, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def f(x):\n",
    "    # transform to tril matrix\n",
    "    r, i = abc(x)\n",
    "\n",
    "    # apply stick-breaking on the squared values\n",
    "    z = r ** 2\n",
    "    z_cumprod = (1 - z).cumprod(-1)\n",
    "\n",
    "    # we omit the step computing s = z * z_cumprod by using the trick:\n",
    "    #     y = sign(r) * s = sign(r) * sqrt(z * z_cumprod) = r * sqrt(z_cumprod)\n",
    "    y = r * defg(F.pad(z_cumprod[..., :-1], pad=(1, 0), value=1), i)\n",
    "    return y\n",
    "\n",
    "\n",
    "def f1(x):\n",
    "    \"\"\"\n",
    "    Because domain and codomain are two spaces with different dimensions, determinant of\n",
    "    Jacobian is not well-defined. Here we return `log_abs_det_jacobian` of `x` and the\n",
    "    flatten lower triangular part of `y`.\n",
    "    \"\"\"\n",
    "    # XXX the fastest way is to return log(y / x) as in StickBreakingTransform;\n",
    "    # however doing so can suffer from the issue x = 0 or y = 0 (which is the same issue\n",
    "    # with StickBreakingTransform's log_abs_det_jacobian);\n",
    "    # fortunately, the probability of having x = 0 or y = 0 is zero, so we will hardly\n",
    "    # suffer this issue in practice.\n",
    "    # Here we will use a slower version but can solve that issue, using the fact that\n",
    "    # y / x = sqrt(z_cumprod)  (modulo right shifted)\n",
    "    r = _vector_to_lower_triangular_with_identity_diagonal(x)\n",
    "\n",
    "    # apply stick-breaking on the squared values\n",
    "    z = r ** 2\n",
    "    z_cumprod = (1 - z).cumprod(-1)\n",
    "\n",
    "    # we omit the step computing s = z * z_cumprod by using the trick:\n",
    "    #     y = sign(r) * s = sign(r) * sqrt(z * z_cumprod) = r * sqrt(z_cumprod)\n",
    "    y = r * F.pad(_masked_sqrt(z_cumprod[..., :-1]), pad=(1, 0), value=1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r = x.new_ones(x.shape[:-1] + (D, D)).tril()\n",
    "tril_index = r.tril(diagonal=-1) > 0.5\n",
    "r[tril_index] = x.reshape(-1)\n",
    "\n",
    "# apply stick-breaking on the squared values\n",
    "z = r ** 2\n",
    "z_cumprod = (1 - z).cumprod(-1)\n",
    "# we omit the step computing s = z * z_cumprod by using the trick:\n",
    "#     y = sign(r) * s = sign(r) * sqrt(z * z_cumprod) = r * sqrt(z_cumprod)\n",
    "\n",
    "# workaround the issue NaN propagated through backward pass even when not accessed\n",
    "# https://github.com/pytorch/pytorch/issues/15506\n",
    "# here we only take sqrt at mask index\n",
    "z_cumprod_sqrt = z_cumprod.new_zeros(z_cumprod.shape)\n",
    "z_cumprod_sqrt[tril_index] = z_cumprod[tril_index].sqrt()        \n",
    "z_cumprod_sqrt_shifted = F.pad(z_cumprod_sqrt[..., :-1], pad=(1, 0), value=1)\n",
    "y = r * z_cumprod_sqrt_shifted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit f1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def _inverse(y):\n",
    "    # inverse stick-breaking\n",
    "    z_cumprod = 1 - y.pow(2).cumsum(-1)\n",
    "    z_cumprod_shifted = F.pad(z_cumprod[..., :-1], pad=(1, 0), value=1)\n",
    "    r = _lower_triangular_to_vector(y, diagonal=-1)\\\n",
    "        / _lower_triangular_to_vector(z_cumprod_shifted, diagonal=-1).sqrt()\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def _inverse3(y):\n",
    "    # inverse stick-breaking\n",
    "    z_cumprod = 1 - y.pow(2).cumsum(-1)\n",
    "    z_cumprod_shifted = F.pad(z_cumprod[..., :-1], pad=(1, 0), value=1)\n",
    "    tril_index = y.new_ones(y.shape).tril(diagonal=-1) > 0.5\n",
    "    r = y[tril_index] / z_cumprod_shifted[tril_index].sqrt()\n",
    "    return r.reshape(y.shape[:-2] + (-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def _inverse1(y):\n",
    "    # inverse stick-breaking\n",
    "    z_cumprod = 1 - y.pow(2).cumsum(-1)\n",
    "    z_cumprod_shifted = F.pad(z_cumprod[..., :-1], pad=(1, 0), value=1)\n",
    "    r = y / z_cumprod_shifted.sqrt()\n",
    "    return _lower_triangular_to_vector(r, diagonal=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def _inverse2(y):\n",
    "    # inverse stick-breaking\n",
    "    z_cumprod = 1 - y.pow(2).cumsum(-1)\n",
    "    r = y / F.pad(_masked_sqrt(z_cumprod[..., :-1]), pad=(1, 0), value=1)\n",
    "    return _lower_triangular_to_vector(r, diagonal=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D = 100\n",
    "value = torch.randn(D, D).tril()\n",
    "value.diagonal(dim1=-2, dim2=-1).exp_()\n",
    "y = value / value.norm(2, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit _inverse3(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit _inverse(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit _inverse1(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit _inverse2(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tril_index = y.new_ones(y.shape).tril(diagonal=-1) > 0.5\n",
    "y_tril_vector = y[tril_index]\n",
    "y_tril_vector / x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def _autograd_log_det(ys, x):\n",
    "    # computes log_abs_det_jacobian of y w.r.t. x\n",
    "    return torch.stack([torch.autograd.grad(y, (x,), retain_graph=True)[0]\n",
    "                        for y in ys]).slogdet()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_autograd_log_det(y_tril_vector, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z_cumprod = 1 - y.pow(2).cumsum(-1)\n",
    "z_cumprod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z_cumprod.sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_tril_vector / x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(y_tril_vector / x).log().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z = r ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z_cumprod = (1 - z).cumprod(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def _masked_sqrt(x):\n",
    "    # hack around the issue NaN propagated through backward pass even when not accessed\n",
    "    # https://github.com/pytorch/pytorch/issues/15506\n",
    "    y = x.new_zeros(x.shape)\n",
    "    mask = x > 0\n",
    "    y[mask] = x[mask].sqrt()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t = F.pad(_masked_sqrt(z_cumprod), pad=(1, 0), value=1)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.autograd.grad(t[0, 0], (x,), retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = torch.zeros(1, requires_grad=True)\n",
    "y = torch.nn.functional.pad(x.sqrt(), pad=(1, 0), value=1)\n",
    "torch.autograd.grad(y[0], (x,))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "#z = x.new_ones(2)\n",
    "#z[1] = x.sqrt()\n",
    "z = torch.cat((x.new_ones(1), x.sqrt()))\n",
    "torch.autograd.grad(z[0], (x,))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.autograd.grad(F.pad(z_cumprod[..., :-1].sqrt(), pad=(1, 0), value=1)[0, 0], (x,), retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply stick-breaking on the squared values\n",
    "z = r ** 2\n",
    "z_cumprod = (1 - z).cumprod(-1)\n",
    "\n",
    "# we omit the step to compute s = z * z_cumprod by using the trick:\n",
    "#     y = sign(r) * s = sign(r) * sqrt(z * z_cumprod) = r * sqrt(z_cumprod)\n",
    "y = r * F.pad(z_cumprod[..., :-1].sqrt(), pad=(1, 0), value=1)\n",
    "return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def _vector_to_lower_triangular_with_identity_diagonal(x):\n",
    "    # make sure that x.size(-1) = D * (D - 1) / 2 for some D\n",
    "    D = (1 + math.sqrt(1 + 8 * x.size(-1))) / 2\n",
    "    if D % 1 != 0:\n",
    "        raise ValueError(\"This transformation requires an input with size D*(D-1)/2.\")\n",
    "    D = int(D)\n",
    "    r = x.new_ones(x.shape[:-1] + (D, D)).tril()\n",
    "    tril_index = r.tril(diagonal=-1) > 0.5\n",
    "    r[tril_index] = x\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def _autograd_log_det(ys, x):\n",
    "    # computes log_abs_det_jacobian of y w.r.t. x\n",
    "    return torch.stack([torch.autograd.grad(y, (x,), retain_graph=True)[0]\n",
    "                        for y in ys]).slogdet()[1].log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.autograd.grad(y[0, 0], (x,), retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_tril_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.stack([torch.autograd.grad(y, (x,), retain_graph=True)[0]\n",
    "                        for y in y_tril_vector])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_autograd_log_det(y_tril_vector, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def _call(x):\n",
    "    # transform to tril matrix\n",
    "    r = _vector_to_lower_triangular_with_identity_diagonal(x)\n",
    "\n",
    "    # apply stick-breaking on the squared values\n",
    "    z = r ** 2\n",
    "    z_cumprod = (1 - z).cumprod(-1).tril()\n",
    "\n",
    "    # we omit the step to compute s = z * z_cumprod by using the trick:\n",
    "    #     y = sign(r) * s = sign(r) * sqrt(z * z_cumprod) = r * sqrt(z_cumprod)\n",
    "    y = r * F.pad(z_cumprod[..., :-1].sqrt(), pad=(1, 0), value=1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F.pad(z_cumprod.sqrt(), pad=(1, 0), value=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_call(torch.rand(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_vector_to_lower_triangular(torch.rand(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.rand(3, 3).tril(diagonal=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D = 100\n",
    "x = torch.rand(10, D, D).tril()\n",
    "x_norm = x.norm(dim=-1)\n",
    "y = x / x_norm.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit f1(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit f2(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f2(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = torch.rand(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit f1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit f2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform = _PartialCorrToCorrCholeskyTransform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = torch.randn((), requires_grad=True)\n",
    "print(x)\n",
    "y = x.tanh()\n",
    "torch.autograd.grad(y.sum(), (x,))[0].log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1 - y**2).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(-2) * y.cosh().log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_shape = (6,)\n",
    "x = torch.rand(x_shape, requires_grad=True) * 2 - 1\n",
    "y = transform(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def _autograd_log_det(y, x):\n",
    "    # computes log_abs_det_jacobian of y w.r.t. x\n",
    "    triu_index = y.new_ones(y.shape).triu(diagonal=1) > 0.5\n",
    "    y_tril_vector = y.t()[triu_index]\n",
    "    return torch.stack([torch.autograd.grad(y, (x,), retain_graph=True)[0] for y in y_tril_vector]).det().abs().log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    D = 4\n",
    "    y = x.new_zeros(x.shape[:-1] + (D, D))\n",
    "    y[..., 0, 0] = 1\n",
    "    y[..., 1:, 0] = x[..., :(D - 1)]\n",
    "    pos_x = D - 1\n",
    "    past_y_squared_sum = None\n",
    "    print(y)\n",
    "    # FIX ME: find a vectorized way to compute y instead of loop\n",
    "    for j in range(1, D):\n",
    "        if j == 1:\n",
    "            past_y_squared_sum = y[..., j:, (j - 1)].pow(2)\n",
    "        else:\n",
    "            past_y_squared_sum = past_y_squared_sum[..., 1:] + y[..., j:, (j - 1)].pow(2)\n",
    "        print(past_y_squared_sum)\n",
    "        y[..., j, j] = (1 - past_y_squared_sum[..., :1]).sqrt()\n",
    "        print(j, j)\n",
    "        print(_autograd_log_det(y, x))\n",
    "        new_pos_x = pos_x + D - 1 - j\n",
    "        y[..., (j + 1):, j] = x[..., pos_x:new_pos_x] * (1 - past_y_squared_sum[..., 1:]).sqrt()\n",
    "        print(range(j+1, D), j)\n",
    "        pos_x = new_pos_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "triu_index = y.new_ones(y.shape).triu(diagonal=1) > 0.5\n",
    "y_tril_vector = y.t()[triu_index]\n",
    "_autograd_log_det(y_tril_vector, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x1 = y.new_ones(y.shape)\n",
    "triu_index = x1.triu(diagonal=1) > 0.5\n",
    "x1[..., :, 0] = y[..., :, 0]\n",
    "x1[..., :, 1:] = y[..., :, 1:] / (1 - y.pow(2).cumsum(-1)[..., :, :-1]).sqrt()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x1.transpose(-1, -2)[triu_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.autograd.grad(y[1,0], (x,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z = transform.inv(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform.domain.check(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = y.new_ones(y.shape)\n",
    "print(x)\n",
    "triu_index = x.triu(diagonal=1) > 0.5\n",
    "x[..., :, 0] = y[..., :, 0]\n",
    "print(x)\n",
    "x[..., :, 1:] = y[..., :, 1:] / (1 - y.pow(2).cumsum(-1)[..., :, :-1]).sqrt()\n",
    "print(x)\n",
    "# we transpose and take upper triangular indices to arrange the result vector\n",
    "# by (x21, x31, x41,..., x32, x42,...) instead of (x21, x31, x32, x41, x42,...)\n",
    "z = x.transpose(-1, -2)[triu_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test codomain\n",
    "assert_tensors_equal(transform.codomain.check(y), torch.ones(x.shape[:-1]))\n",
    "\n",
    "# test inv\n",
    "z = transform.inv(y)\n",
    "assert_tensors_equal(x, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.arange(10).reshape(2, 5).cumsum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.rand(3, 3).tril(diagonal=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = torch.rand(3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1 - x.pow(2).cumsum(-1)).tril(diagonal=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D = 4\n",
    "x = torch.rand(D, D)\n",
    "\n",
    "def f1(x):\n",
    "    y = x.new_zeros(x.shape)\n",
    "    y[..., 0, :] = x[..., 0, :]\n",
    "    y[..., 1:, :] = x[..., 1:, :] / (1 - x.pow(2).cumsum(-1)[..., :-1, :]).sqrt()\n",
    "    triu_index = x.new_ones(D, D).triu(diagonal=1) > 0.5\n",
    "    return y.t()[triu_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def _inverse(x):\n",
    "    if (x.shape[0] != x.shape[1]):\n",
    "        raise ValueError(\"A matrix that isn't square can't be a Cholesky factor of a correlation matrix\")\n",
    "    D = x.shape[0]\n",
    "\n",
    "    z_stack = [\n",
    "        x[1:, 0]\n",
    "    ]\n",
    "    current_x = z_stack[0]\n",
    "    last_squared_x = None\n",
    "    for j in range(1, D):\n",
    "        if last_squared_x is None:\n",
    "            last_squared_x = current_x**2\n",
    "        else:\n",
    "            last_squared_x += current_x[1:]**2\n",
    "    current_x = x[j:, j]\n",
    "    z_stack.append(current_x / (1 - last_squared_x).sqrt())\n",
    "    z = torch.cat(z_stack)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_inverse(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%timeit f1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x.pow(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x.triu(diagonal=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x.pow(2).cumsum(-1)[..., :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x[..., 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x[..., :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y[:, 0] = x[..., 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
