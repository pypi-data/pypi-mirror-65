Metadata-Version: 2.1
Name: nevergrad
Version: 0.4.0.post3
Summary: A Python toolbox for performing gradient-free optimization
Home-page: https://github.com/facebookresearch/nevergrad
Author: Facebook AI Research
License: MIT
Platform: UNKNOWN
Classifier: License :: OSI Approved :: MIT License
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering
Classifier: Programming Language :: Python
Requires-Python: >=3.6
Description-Content-Type: text/markdown
Requires-Dist: pandas (>=0.23.4)
Requires-Dist: numpy (>=1.15.0)
Requires-Dist: cma (>=2.6.0)
Requires-Dist: bayesian-optimization (>=1.1.0)
Requires-Dist: typing-extensions (>=3.6.6)
Provides-Extra: all
Requires-Dist: mypy (>=0.730) ; extra == 'all'
Requires-Dist: pytest (>=4.3.0) ; extra == 'all'
Requires-Dist: pytest-cov (>=2.6.1) ; extra == 'all'
Requires-Dist: wheel (>=0.33.6) ; extra == 'all'
Requires-Dist: setuptools (>=41.2.0) ; extra == 'all'
Requires-Dist: sphinx (>=2.1.0) ; extra == 'all'
Requires-Dist: sphinx-rtd-theme (>=0.4.3) ; extra == 'all'
Requires-Dist: recommonmark (>=0.5.0) ; extra == 'all'
Requires-Dist: twine (>=3.1.1) ; extra == 'all'
Requires-Dist: autodocsumm (>=0.1.11) ; extra == 'all'
Requires-Dist: requests (>=2.21.0) ; extra == 'all'
Requires-Dist: xlwt (>=1.3.0) ; extra == 'all'
Requires-Dist: xlrd (>=1.2.0) ; extra == 'all'
Requires-Dist: matplotlib (>=2.2.3) ; extra == 'all'
Requires-Dist: gym (>=0.12.1) ; extra == 'all'
Requires-Dist: torch (>=1.2.0) ; extra == 'all'
Requires-Dist: hiplot ; extra == 'all'
Requires-Dist: fcmaes (>=0.9.5.6) ; extra == 'all'
Requires-Dist: tqdm ; extra == 'all'
Provides-Extra: benchmark
Requires-Dist: requests (>=2.21.0) ; extra == 'benchmark'
Requires-Dist: xlwt (>=1.3.0) ; extra == 'benchmark'
Requires-Dist: xlrd (>=1.2.0) ; extra == 'benchmark'
Requires-Dist: matplotlib (>=2.2.3) ; extra == 'benchmark'
Requires-Dist: gym (>=0.12.1) ; extra == 'benchmark'
Requires-Dist: torch (>=1.2.0) ; extra == 'benchmark'
Requires-Dist: hiplot ; extra == 'benchmark'
Requires-Dist: fcmaes (>=0.9.5.6) ; extra == 'benchmark'
Requires-Dist: tqdm ; extra == 'benchmark'
Provides-Extra: dev
Requires-Dist: mypy (>=0.730) ; extra == 'dev'
Requires-Dist: pytest (>=4.3.0) ; extra == 'dev'
Requires-Dist: pytest-cov (>=2.6.1) ; extra == 'dev'
Requires-Dist: wheel (>=0.33.6) ; extra == 'dev'
Requires-Dist: setuptools (>=41.2.0) ; extra == 'dev'
Requires-Dist: sphinx (>=2.1.0) ; extra == 'dev'
Requires-Dist: sphinx-rtd-theme (>=0.4.3) ; extra == 'dev'
Requires-Dist: recommonmark (>=0.5.0) ; extra == 'dev'
Requires-Dist: twine (>=3.1.1) ; extra == 'dev'
Requires-Dist: autodocsumm (>=0.1.11) ; extra == 'dev'

[![CircleCI](https://circleci.com/gh/facebookresearch/nevergrad/tree/master.svg?style=svg)](https://circleci.com/gh/facebookresearch/nevergrad/tree/master)

# Nevergrad - A gradient-free optimization platform

![Nevergrad](https://raw.githubusercontent.com/facebookresearch/nevergrad/master/docs/resources/Nevergrad-LogoMark.png)


`nevergrad` is a Python 3.6+ library. It can be installed with:

```
pip install nevergrad
```

More installation options and complete instructions are available in the "Getting started" section of the [**documentation**](https://facebookresearch.github.io/nevergrad/).

You can join Nevergrad users Facebook group [here](https://www.facebook.com/groups/nevergradusers/).

Minimizing a function using an optimizer (here `OnePlusOne`) is straightforward:

```python
import nevergrad as ng

def square(x):
    return sum((x - .5)**2)

optimizer = ng.optimizers.OnePlusOne(parametrization=2, budget=100)
recommendation = optimizer.minimize(square)
print(recommendation)  # optimal args and kwargs
>>> Array{(2,)}[recombination=average,sigma=1.0]:[0.49971112 0.5002944 ]
```

![Example of optimization](https://raw.githubusercontent.com/facebookresearch/nevergrad/master/docs/resources/TwoPointsDE.gif)

*Convergence of a population of points to the minima with two-points DE.*


## Documentation

Check out our [**documentation**](https://facebookresearch.github.io/nevergrad/)! It's still a work in progress, don't hesitate to submit issues and/or PR to update it and make it clearer!


## Citing

```bibtex
@misc{nevergrad,
    author = {J. Rapin and O. Teytaud},
    title = {{Nevergrad - A gradient-free optimization platform}},
    year = {2018},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://GitHub.com/FacebookResearch/Nevergrad}},
}
```

## License

`nevergrad` is released under the MIT license. See [LICENSE](https://github.com/facebookresearch/nevergrad/blob/master/LICENSE) for additional details about it.


