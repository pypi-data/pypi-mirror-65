#!/usr/bin/env python3
import argparse
import os
import hashlib
import shutil
from tqdm import tqdm

def megachonker(in_file, out_file, chunk_size=10**6, quiet=False):
    """
    compares mtime of in_file and out_file, then open file, read contents 
    in binary, NOMINALLY: chunk it up into 1MB chunks
    """
    if not os.path.exists(out_file):
        with open(out_file, 'wb') as f:
            pass
    mtime_in = os.path.getmtime(in_file)
    mtime_out = os.path.getmtime(out_file)
    if mtime_in == mtime_out:
        # print("No recent changes, skipping file.")
        return

    # print("Chonking file.")
    old_hash_file = out_file + ".hashes"
    new_hash_file = out_file + ".hashes.temp"
    # check if exists else create out_file and old_hashes
    if not os.path.exists(old_hash_file):
        with open(old_hash_file, 'wb') as f:
            pass

    if not quiet:
        progress_bar = tqdm(
            total=os.path.getsize(in_file),
            smoothing=0,
            unit='B',
            unit_scale=True,
            dynamic_ncols=True
        )
    
    # opening all them files
    with open(in_file, 'rb') as f_in:
        with open(out_file, 'r+b') as f_out:
            with open(old_hash_file, 'r+b') as g_old:
                with open(new_hash_file, 'wb') as g_new:
                    while True:
                        digest_old = g_old.read(32)

                        chunk_start = f_in.tell()
                        # current chunk size is: 1 MB
                        chunk = f_in.read(chunk_size)
                        # if no more in_file
                        if not chunk:
                            # ensure files are of the same size
                            f_in_end = f_in.tell()    
                            f_out.truncate(f_in_end)
                            break

                        # creating a hash for chunk
                        digest = hashlib.sha256(chunk).digest()
                        # writing into new has file
                        g_new.write(digest)

                        if digest != digest_old:
                            f_out.seek(chunk_start)
                            f_out.write(chunk)
                        if not quiet:
                            progress_bar.update(len(chunk))

    shutil.copystat(in_file, out_file)
    os.rename(new_hash_file, old_hash_file)

if __name__ == '__main__':

    parser = argparse.ArgumentParser(description="""
        Use megachonker for optimizing the backup of large files. megachonker
        dissects in_file into 1MB chunks and hashes each chunk.  Each hash is
        then compared to a hash for the respective chunk in out_file.  The
        chunk gets copied into out_file if hashes differ. A file
        <out_file>.hashes will be stored alongside out_file. megachonker
        copies all file attributes of in_file to out_file. If in_file and
        out_file have identical modification times then megachonker does
        nothing.""")
    
    parser.add_argument(
        '-q',
        '--quiet',
        action='store_true',
        default=False,
        help="Do not show any output.",
    )

    parser.add_argument("in_file", help="This is the file to be chunked, hashed and copied")
    parser.add_argument("out_file", help="This is the backed up version of the file")
    args = parser.parse_args()

    megachonker(args.in_file, args.out_file, quiet=args.quiet)