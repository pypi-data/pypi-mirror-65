# sinkhorn-transformer

Sinkhorn Transformers, made even more efficient with reversible networks and chunking (from Reformer). Code coming soon

https://arxiv.org/abs/2002.11296

## Citations

```bibtex
@misc{tay2020sparse,
    title={Sparse Sinkhorn Attention},
    author={Yi Tay and Dara Bahri and Liu Yang and Donald Metzler and Da-Cheng Juan},
    year={2020},
    eprint={2002.11296},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
```

```bibtex
@inproceedings{kitaev2020reformer,
    title       = {Reformer: The Efficient Transformer},
    author      = {Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
    booktitle   = {International Conference on Learning Representations},
    year        = {2020},
    url         = {https://openreview.net/forum?id=rkgNKkHtvB}
}
```
